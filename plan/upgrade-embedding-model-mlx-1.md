---
goal: Replace sentence-transformers with mlx-embeddings using EmbeddingGemma-300m for both paragraph segmentation and tag extraction
version: 1.0
date_created: 2026-02-08
last_updated: 2026-02-08
owner: Core App Team
status: Planned
tags: [upgrade, architecture, python, mlx, embeddings, performance, dependency-reduction]
---

# Introduction

![Status: Planned](https://img.shields.io/badge/status-Planned-blue)

This plan migrates WhisperDesk's two embedding models — `all-MiniLM-L6-v2` (384-dim, paragraph segmentation) and `all-mpnet-base-v2` (768-dim, tag extraction / theme matching) — to a single unified model: **EmbeddingGemma-300m** (`mlx-community/embeddinggemma-300m-4bit`) running on the `mlx-embeddings` package. This eliminates PyTorch (~2GB), `sentence-transformers`, and `keybert` as dependencies, replacing them with a single native Apple Silicon MLX embedding model that produces 768-dim embeddings with an MTEB score of ~69.67 (vs ~56 and ~63 for the current models).

Both paragraph segmentation and tag extraction will use the same EmbeddingGemma-300m model instance (loaded once, reused). The `mlx-embeddings` API differs from `sentence-transformers` — it uses `tokenizer.batch_encode_plus()` + `model()` instead of `model.encode()` — so all call sites must be adapted. KeyBERT is also removed entirely, as the semantic KB matching already provides superior theme inference.

**Key outcomes:**

- Single unified embedding model for all embedding operations
- ~2GB dependency reduction (PyTorch + sentence-transformers + keybert removed)
- Better embedding quality (MTEB 69.67 vs 56/63)
- Native MLX GPU acceleration (no MPS translation layer)
- 2048 token context (4× the current 512)
- Task-specific prompting support from EmbeddingGemma

## 1. Requirements & Constraints

- **REQ-001**: Replace `all-MiniLM-L6-v2` (paragraph segmentation) with `EmbeddingGemma-300m-4bit` via `mlx-embeddings`.
- **REQ-002**: Replace `all-mpnet-base-v2` (tag extraction / theme matching) with the same `EmbeddingGemma-300m-4bit` model.
- **REQ-003**: Use a single shared model instance for both operations — load once, reuse everywhere.
- **REQ-004**: Remove `torch`, `sentence-transformers`, and `keybert` from `requirements.txt` and all import sites.
- **REQ-005**: Remove all PyTorch device detection logic (`torch.backends.mps`, MPS `.to(device)` calls).
- **REQ-006**: Adapt all `.encode()` call sites to use `mlx-embeddings` API: `tokenizer.batch_encode_plus()` → `model()` → `outputs.text_embeds`.
- **REQ-007**: Output embeddings must be numpy arrays for downstream cosine similarity computations with `np.dot` / `np.linalg.norm`.
- **REQ-008**: Paragraph segmentation similarity threshold may need retuning for the new 768-dim embeddings (was calibrated for 384-dim MiniLM).
- **REQ-009**: Tag extraction semantic threshold may need retuning for the higher-quality embeddings.
- **REQ-010**: KeyBERT explicit keyword extraction is removed entirely — the theological KB semantic inference is the sole tag extraction method.
- **REQ-011**: NLTK remains unchanged (POS tagging is unaffected by this migration).
- **REQ-012**: All existing Python tests must pass or be updated to reflect the new model.
- **REQ-013**: The Electron installer (`python-installer.ts`) must be updated to install `mlx-embeddings` and remove `torch`/`sentence-transformers`/`keybert`.
- **REQ-014**: Dependency verification in `whisper_bridge.py` (`check_dependencies`) must check for `mlx_embeddings` instead of `torch`/`sentence_transformers`/`keybert`.

- **CON-001**: No legacy code paths — this is a full cutover, not a feature flag / gradual migration.
- **CON-002**: The `mlx-embeddings` package only works on Apple Silicon Macs (which is the exclusive target platform).
- **CON-003**: EmbeddingGemma-300m outputs 768-dim embeddings. The paragraph segmentation code previously used 384-dim. Cosine similarity math is dimension-agnostic, so no formula changes are needed, but thresholds may differ.
- **CON-004**: The `mlx-embeddings` `text_embeds` output is an MLX array — must be converted to numpy via `np.array()` for existing `np.dot`/`np.linalg.norm` math.

- **GUD-001**: Keep the embedding wrapper function simple and centralized — one function to encode texts, used by all consumers.
- **GUD-002**: EmbeddingGemma supports task-specific prompts (`semantic_similarity`, `classification`, `clustering`). Use `semantic_similarity` for paragraph segmentation and `classification` for tag extraction to leverage task-optimized embeddings.
- **GUD-003**: Cache the model/tokenizer globally (lazy-load on first use) — same pattern as the current `tag_model` lazy loading.

- **PAT-001**: Follow the existing pattern of global model singletons with lazy initialization in `main.py`.
- **PAT-002**: Follow the existing pattern of `get_semantic_model()` / `get_tag_model()` accessor functions in `whisper_bridge.py`, but consolidate into a single `get_embedding_model()`.

## 2. Implementation Steps

### Implementation Phase 1: Create Unified Embedding Module

- GOAL-001: Create a new `src/python/embedding_model.py` module that encapsulates all `mlx-embeddings` logic in one place — model loading, text encoding, numpy conversion. All other modules import from here.

| Task     | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Completed | Date |
| -------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | ---- |
| TASK-001 | Create `src/python/embedding_model.py` with these components: (a) Global `_model` and `_tokenizer` variables (initially `None`). (b) `MODEL_NAME = "mlx-community/embeddinggemma-300m-4bit"` constant. (c) `load_model()` function that lazily loads via `from mlx_embeddings.utils import load; model, tokenizer = load(MODEL_NAME)`. (d) `encode_texts(texts: List[str], task: str = "semantic_similarity") -> np.ndarray` function that tokenizes via `tokenizer.batch_encode_plus(texts, return_tensors="mlx", padding=True, truncation=True, max_length=2048)`, runs `model(input_ids, attention_mask=attention_mask)`, extracts `outputs.text_embeds`, and converts to numpy via `np.array(outputs.text_embeds)`. (e) `encode_single(text: str, task: str = "semantic_similarity") -> np.ndarray` convenience function wrapping `encode_texts`. |           |      |
| TASK-002 | In `encode_texts()`, handle EmbeddingGemma task-specific prompting. EmbeddingGemma uses task prefixes internally — verify whether `mlx-embeddings` exposes this or if we need to prepend task tokens manually. If the `mlx-community/embeddinggemma-300m-4bit` checkpoint handles task prompting through the model config, no manual prefix is needed. Test with a simple script to confirm `outputs.text_embeds` shape is `(batch_size, 768)` and values are normalized.                                                                                                                                                                                                                                                                                                                                                                             |           |      |
| TASK-003 | Add batch size management to `encode_texts()`. For paragraph segmentation, input can be 200+ sentences. Process in batches of 64 and concatenate results with `np.vstack()` to avoid OOM. Add a `batch_size: int = 64` parameter.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |           |      |

### Implementation Phase 2: Migrate `main.py` — Paragraph Segmentation

- GOAL-002: Replace all `semantic_model` (MiniLM) usage in `segment_into_paragraphs()` with the new `embedding_model.encode_texts()`.

| Task     | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Completed | Date |
| -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------- | ---- |
| TASK-004 | Remove the top-level imports and initialization in `main.py`: delete `import torch` (lines 1), the torch device detection block (lines 13-19), `from sentence_transformers import SentenceTransformer` (line 25), and the `semantic_model` loading block (lines 59-64: `semantic_model = SentenceTransformer(...)` / `.to(device)`). Replace with `from embedding_model import encode_texts, load_model`.                                                                                                                                                                                                                                                                            |           |      |
| TASK-005 | In `segment_into_paragraphs()` function (around line 720), replace `embeddings = semantic_model.encode(sentences, convert_to_numpy=True)` with `embeddings = encode_texts(sentences, task="semantic_similarity")`. The returned numpy array is the same shape `(N, 768)` — the cosine similarity math (`np.dot`, `np.linalg.norm`) works unchanged.                                                                                                                                                                                                                                                                                                                                  |           |      |
| TASK-006 | Re-tune the `similarity_threshold` parameter for paragraph segmentation. The default is currently `0.65` (function default) / `0.30` (pipeline call). EmbeddingGemma-300m produces higher-quality embeddings with different similarity distributions. Run the test transcript through the pipeline with the new model and compare paragraph counts. Adjust the default `similarity_threshold` in the function signature and the pipeline call in `__main__` and `whisper_bridge.py` `segment_paragraphs()`. **Approach**: Start with the current threshold, test, and if paragraphs are too many/few, adjust by ±0.05 increments. Document the final chosen value in a code comment. |           |      |

### Implementation Phase 3: Migrate `main.py` — Tag Extraction & Theme Matching

- GOAL-003: Replace all `tag_model` (mpnet) usage in tag extraction functions with the unified `embedding_model.encode_texts()`, and remove KeyBERT entirely.

| Task     | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Completed | Date |
| -------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | ---- |
| TASK-007 | Remove the `TAG_MODEL_NAME`, `tag_model`, and `religious_embedding` global variables (lines 66-69). Remove all `tag_model = SentenceTransformer(TAG_MODEL_NAME)` and `.to(device)` lazy-loading blocks throughout the file (in `compute_religious_embedding()`, `get_religious_relevance()`, `compute_concept_embeddings()`, `get_semantic_themes()`, `extract_tags()`).                                                                                                                      |           |      |
| TASK-008 | Rewrite `compute_religious_embedding()` (around line 830): Replace `tag_model.encode(RELIGIOUS_SEED_CONCEPTS)` with `encode_texts(RELIGIOUS_SEED_CONCEPTS, task="classification")`. Return `np.mean(embeddings, axis=0)` as before. Update the global cache variable `religious_embedding`.                                                                                                                                                                                                   |           |      |
| TASK-009 | Rewrite `get_religious_relevance()` (around line 855): Replace `tag_model.encode([word])[0]` with `encode_texts([word], task="classification")[0]`. Cosine similarity math is unchanged.                                                                                                                                                                                                                                                                                                      |           |      |
| TASK-010 | Rewrite `compute_concept_embeddings()` (around line 880): Replace `tag_model.encode(THEOLOGICAL_CONCEPTS_KB)` with `encode_texts(THEOLOGICAL_CONCEPTS_KB, task="classification")`. Cache result in `theological_concept_embeddings` global as before.                                                                                                                                                                                                                                         |           |      |
| TASK-011 | Rewrite `get_semantic_themes()` (around line 940): Replace `tag_model.encode(chunks)` with `encode_texts(chunks, task="classification")`. The rest of the function (sermon-level embedding averaging, per-chunk similarity, 60/40 weighting) remains unchanged.                                                                                                                                                                                                                               |           |      |
| TASK-012 | Remove KeyBERT entirely from `extract_tags()` (around line 1100-1260): Delete the `KEYBERT_AVAILABLE` import guard at the top of `main.py` (lines 30-35). Delete the entire "STEP 2: EXPLICIT KEYWORD EXTRACTION" block from `extract_tags()` — this includes the KeyBERT model creation, noun extraction, keyword filtering, religious relevance scoring, and `explicit_candidates` logic. The function should now **only** contain STEP 1 (semantic inference via `get_semantic_themes()`). |           |      |
| TASK-013 | Simplify `extract_tags()` signature: Remove parameters that only applied to KeyBERT (`min_occurrences`, `min_keybert_score`, `min_religious_relevance`, `nouns_only`). Keep: `text`, `quote_boundaries`, `max_tags`, `verbose`, `use_semantic_inference`, `semantic_threshold`. The `COMMON_STOP_WORDS`, `BIBLE_BOOK_NAMES`, `is_noun()`, `extract_nouns_from_text()` functions can be removed as they were only used by KeyBERT filtering.                                                   |           |      |
| TASK-014 | Re-evaluate the semantic threshold for tag extraction. The current `semantic_threshold=0.40` (default) and `min_similarity=0.35` (in `get_semantic_themes()`) were calibrated for mpnet. EmbeddingGemma-300m will likely produce different similarity distributions. Test with the test transcript and compare tag output quality. Adjust thresholds. Document the final values.                                                                                                              |           |      |

### Implementation Phase 4: Migrate `whisper_bridge.py`

- GOAL-004: Update the subprocess bridge to use the new unified embedding model and remove all torch/sentence-transformers references.

| Task     | Description                                                                                                                                                                                                                                                                                                                                   | Completed | Date |
| -------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | ---- |
| TASK-015 | Remove `_semantic_model`, `_tag_model`, `_device` globals (lines 161-163). Remove `get_device()` function (lines 215-230) — it only existed for torch MPS detection. Remove `get_semantic_model()` function (lines 226-237). Remove `get_tag_model()` function (lines 240-250).                                                               |           |      |
| TASK-016 | Add a single `get_embedding_model()` function that imports from `embedding_model.py`: `from embedding_model import load_model; return load_model()`. This keeps the bridge's lazy-loading pattern but delegates to the centralized module.                                                                                                    |           |      |
| TASK-017 | Update `check_dependencies()` function (around line 1030): Remove checks for `torch`, `sentence_transformers`, `keybert`. Add check for `mlx_embeddings`: `from mlx_embeddings.utils import load; deps['mlx_embeddings'] = True`. Remove `torch_device`, `mps_available`, `cuda_available` from the return dict. Update `all_installed` list. |           |      |
| TASK-018 | Update the `segment_paragraphs()` function (around line 700): It currently imports `segment_into_paragraphs` from `main.py`, which now uses `embedding_model` internally — no direct changes needed here, but verify the import chain works.                                                                                                  |           |      |
| TASK-019 | Update the `extract_tags()` function (around line 740): It currently imports `extract_tags` from `main.py` — same as TASK-018, verify the import chain. Ensure the simplified `extract_tags()` signature is compatible with the call from `whisper_bridge.py`.                                                                                |           |      |

### Implementation Phase 5: Update Electron Installer & Dependencies

- GOAL-005: Update `requirements.txt` and `python-installer.ts` to install `mlx-embeddings` instead of `torch`/`sentence-transformers`/`keybert`.

| Task     | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Completed | Date                                                       |
| -------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | ---------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ | --- | --- |
| TASK-020 | Update `src/python/requirements.txt`: Remove `torch>=2.0.0`, `sentence-transformers>=2.2.0`, `transformers>=4.30.0`, `keybert>=0.8.0`. Add `mlx-embeddings>=0.1.0`. Keep `mlx-whisper>=0.4.0`, `nltk>=3.8.0`, `mutagen>=1.47.0`, `requests>=2.31.0`, `numpy>=1.24.0`.                                                                                                                                                                                                                                                                                                                                                       |           |                                                            |
| TASK-021 | Update `src/main/services/python-installer.ts` `checkPythonStatus()` (around line 235): Change the package verification command from `'import mlx_whisper; import sentence_transformers; import keybert'` to `'import mlx_whisper; import mlx_embeddings'`.                                                                                                                                                                                                                                                                                                                                                                 |           |                                                            |
| TASK-022 | Update `src/main/services/python-installer.ts` `installPackages()` (around lines 340-390): Remove the separate PyTorch installation step (`pip install torch torchvision torchaudio`). Remove the `Installing PyTorch for semantic analysis...` progress message. Update the final verification command (line 377) from `'import mlx_whisper; import sentence_transformers; import keybert'` to `'import mlx_whisper; import mlx_embeddings'`. Update progress messages to reflect `mlx-embeddings` instead of PyTorch. Adjust progress percentages since the PyTorch install step (which was 20% of the total) is removed. |           |                                                            |
| TASK-023 | Update `src/main/services/python-installer.ts` device detection in `checkPythonStatus()` (around lines 242-265): Remove the torch-based device detection fallback. The device is always `mlx` on Apple Silicon. Simplify to just check for MLX availability.                                                                                                                                                                                                                                                                                                                                                                |           |                                                            |
| TASK-024 | Clean up `PythonStatus` type if needed — the `device` field type currently includes `'mps'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 'cuda'    | 'cpu'`. Since only `'mlx'`is relevant now, update to`'mlx' | 'cpu'`(keep`cpu`as fallback for edge cases). Search for any TypeScript code that checks`device === 'mps'`or`device === 'cuda'` and update. |     |     |

### Implementation Phase 6: Clean Up Removed Code

- GOAL-006: Remove all dead code, unused imports, and orphaned functions that only existed for the old embedding pipeline.

| Task     | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Completed | Date |
| -------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | ---- |
| TASK-025 | In `main.py`, remove `compute_religious_embedding()` and `get_religious_relevance()` functions entirely — they were only used by the KeyBERT filtering path in `extract_tags()` which is now removed. Also remove the `RELIGIOUS_SEED_CONCEPTS` list and the `religious_embedding` global variable.                                                                                                                                                                                                                                                                                           |           |      |
| TASK-026 | In `main.py`, remove `COMMON_STOP_WORDS`, `BIBLE_BOOK_NAMES` sets, `is_noun()`, `extract_nouns_from_text()` functions — all were exclusively used by the removed KeyBERT explicit extraction path.                                                                                                                                                                                                                                                                                                                                                                                            |           |      |
| TASK-027 | In `main.py`, remove the NLTK import guard block (lines 40-55: `try: import nltk...`) if NLTK is no longer used anywhere after KeyBERT removal. **Check first**: if `is_noun()` and `extract_nouns_from_text()` are removed, and no other function uses NLTK, then remove the import. If `convert_to_markdown()` or other functions still use NLTK for anything, keep it. Based on code analysis: NLTK was only used for POS tagging in the KeyBERT path, so it can be removed from `main.py`. It may still be needed by `whisper_bridge.py` — check before removing from `requirements.txt`. |           |      |
| TASK-028 | In `main.py`, remove the MLX detection block at the top (lines 3-10: `try: import mlx.core...`) — this was informational only, and the embedding module handles MLX internally. Keep the SSL fix (lines 22-23).                                                                                                                                                                                                                                                                                                                                                                               |           |      |
| TASK-029 | Verify no other files import removed symbols from `main.py`. Run: `grep -rn "from main import\|import main" src/python/` and check each import is still valid. Specifically, `whisper_bridge.py` imports `segment_into_paragraphs` and `extract_tags` from `main.py` — ensure those functions still exist with compatible signatures.                                                                                                                                                                                                                                                         |           |      |

### Implementation Phase 7: Download EmbeddingGemma Model

- GOAL-007: Ensure the EmbeddingGemma-300m-4bit model is downloaded during setup and available at runtime.

| Task     | Description                                                                                                                                                                                                                                                                                                                                                                     | Completed | Date |
| -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | ---- |
| TASK-030 | The `mlx-embeddings` `load()` function auto-downloads models from HuggingFace Hub on first use. Verify this works with the `HF_HOME` environment variable that `python-installer.ts` sets (pointed at the `models/huggingface` directory). The model will be cached in `models/huggingface/hub/models--mlx-community--embeddinggemma-300m-4bit/`.                               |           |      |
| TASK-031 | Add a pre-download step to `python-installer.ts` `installPackages()` or create a new `downloadEmbeddingModel()` function. After installing packages, run: `python3 -c "from mlx_embeddings.utils import load; load('mlx-community/embeddinggemma-300m-4bit')"` to pre-cache the model. This prevents a long delay on first transcription. Add progress reporting for this step. |           |      |
| TASK-032 | Delete the old sentence-transformers model caches from `models/huggingface/hub/`: `models--sentence-transformers--all-MiniLM-L6-v2` and `models--sentence-transformers--all-mpnet-base-v2`. These are no longer needed and reclaim disk space. Add this as a cleanup step in the installer or document it as a manual step.                                                     |           |      |

### Implementation Phase 8: Testing

- GOAL-008: Comprehensive testing to verify the migration produces correct results and nothing is broken.

| Task     | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | Completed | Date |
| -------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | ---- |
| TASK-033 | Create `src/python/test_embedding_model.py`: Unit tests for the new `embedding_model.py` module. Tests: (a) `test_load_model()` — verify model and tokenizer load without error. (b) `test_encode_single_text()` — verify output shape is `(1, 768)` and dtype is float. (c) `test_encode_batch()` — verify batch of 5 texts produces `(5, 768)`. (d) `test_encode_large_batch()` — verify 100+ texts works with internal batching. (e) `test_embeddings_are_normalized()` — verify `np.linalg.norm(emb)` ≈ 1.0. (f) `test_similar_texts_have_high_similarity()` — "God is love" and "The Lord loves us" should have cosine sim > 0.7. (g) `test_dissimilar_texts_have_low_similarity()` — "God is love" and "The weather is sunny" should have cosine sim < 0.5. |           |      |
| TASK-034 | Create `src/python/test_paragraph_segmentation.py`: Integration test for paragraph segmentation with the new model. Tests: (a) `test_basic_segmentation()` — a multi-topic text produces at least 2 paragraphs. (b) `test_single_topic_not_split()` — a short coherent text stays as one paragraph. (c) `test_quote_boundaries_respected()` — quotes are never split across paragraphs (existing behavior must be preserved). (d) `test_prayer_detection_still_works()` — prayers get their own paragraphs (existing behavior). (e) `test_threshold_sensitivity()` — lowering threshold produces more paragraphs, raising it produces fewer.                                                                                                                      |           |      |
| TASK-035 | Create `src/python/test_tag_extraction.py`: Integration test for tag extraction with the new model. Tests: (a) `test_basic_theme_extraction()` — a text about "following Jesus" and "denying self" returns "Discipleship" in tags. (b) `test_returns_multiple_themes()` — a complex sermon text returns between 3-10 themes. (c) `test_excludes_quoted_bible_text()` — quoted Bible text (via `quote_boundaries`) is excluded from analysis. (d) `test_max_tags_respected()` — setting `max_tags=5` returns at most 5 tags. (e) `test_theological_concepts_kb_coverage()` — all concepts in the KB can be encoded without error. (f) `test_no_keybert_dependency()` — importing `extract_tags` does not trigger `keybert` import.                                 |           |      |
| TASK-036 | Create `src/python/test_whisper_bridge_deps.py`: Test the dependency check and model loading paths. Tests: (a) `test_check_dependencies_reports_mlx_embeddings()` — `check_dependencies()` returns `mlx_embeddings: True`. (b) `test_check_dependencies_no_torch()` — `check_dependencies()` does not check or report on `torch`. (c) `test_check_dependencies_no_keybert()` — `check_dependencies()` does not check or report on `keybert`.                                                                                                                                                                                                                                                                                                                      |           |      |
| TASK-037 | Run the existing test suite (`test_e2e_pipeline.py`, `test_ast_passage_boundaries.py`, `test_boundary_detection.py`, `test_passage_isolation.py`, `test_remap_boundaries.py`) to verify no regressions. These tests focus on Bible quote processing and AST building, which are unaffected by embedding changes — but verify they don't accidentally import removed symbols.                                                                                                                                                                                                                                                                                                                                                                                      |           |      |
| TASK-038 | End-to-end smoke test: Run the full `process_sermon` pipeline via `whisper_bridge.py` with `skip_transcription=True` (uses `test_mode_transcript.txt`). Verify: (a) No import errors. (b) Paragraphs are produced. (c) Tags are produced and are reasonable theological themes. (d) AST is built successfully. (e) No references to torch/sentence-transformers/keybert in stderr output. (f) Progress events are emitted correctly for all stages. Capture the output and compare tag quality to a baseline from the current model.                                                                                                                                                                                                                              |           |      |
| TASK-039 | Run the Vitest test suite (`npm run test:run`) to verify no TypeScript regressions. Focus on: any tests that mock or reference `PythonStatus.device`, dependency check responses, or the installer flow.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |           |      |

### Implementation Phase 9: Threshold Calibration & Documentation

- GOAL-009: Calibrate similarity thresholds for the new model and document the migration.

| Task     | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | Completed | Date |
| -------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------- | ---- |
| TASK-040 | Create a calibration script `src/python/calibrate_thresholds.py` that: (a) Loads the test transcript. (b) Runs paragraph segmentation with thresholds from 0.20 to 0.80 in 0.05 increments. (c) Prints paragraph count for each threshold. (d) Runs tag extraction with `min_similarity` from 0.25 to 0.60 in 0.05 increments. (e) Prints tag results for each threshold. (f) Compares results to a known-good baseline. Use this to determine final production threshold values. |           |      |
| TASK-041 | Update code comments in `main.py` to document the model choice, dimension change (384→768 for paragraph segmentation), and threshold rationale. Update the module docstring to reflect the new architecture.                                                                                                                                                                                                                                                                      |           |      |
| TASK-042 | Update the `Model Research Report.md` to mark Priority 1 (embedding migration) as completed and note the specific model/threshold choices made.                                                                                                                                                                                                                                                                                                                                   |           |      |

## 3. Alternatives

- **ALT-001**: Use `bge-small-en-v1.5` for paragraph segmentation (384-dim, closer drop-in) and `EmbeddingGemma-300m` only for tags. Rejected because: using two separate models defeats the simplification goal, and EmbeddingGemma handles both tasks with higher quality.
- **ALT-002**: Use `gte-modernbert-base` (149M params, 768-dim, MTEB 64.38). Rejected because: EmbeddingGemma-300m scores significantly higher (MTEB 69.67) and supports task-specific prompts.
- **ALT-003**: Keep KeyBERT and wrap the MLX embedding model as a KeyBERT backend. Rejected because: KeyBERT is largely redundant given the 200+ concept theological KB, and removing it eliminates a dependency.
- **ALT-004**: Use the full `EmbeddingGemma-300m` (non-quantized bf16). Rejected because: the 4-bit quantized version has minimal quality loss with much smaller disk footprint, and `mlx-embeddings` supports it natively.

## 4. Dependencies

- **DEP-001**: `mlx-embeddings` — Python package from Blaizzy (same ecosystem as `mlx-whisper`). Install via `pip install mlx-embeddings`.
- **DEP-002**: `mlx-community/embeddinggemma-300m-4bit` — HuggingFace model checkpoint. Auto-downloaded by `mlx-embeddings` on first use. Approximate disk size: ~200MB (4-bit quantized).
- **DEP-003**: `mlx>=0.8.0` — Already installed as a dependency of `mlx-whisper`. `mlx-embeddings` also depends on it.
- **DEP-004**: `numpy>=1.24.0` — Already installed. Used for cosine similarity math after converting MLX arrays to numpy.
- **DEP-005**: Removal of `torch>=2.0.0` (~2GB), `sentence-transformers>=2.2.0`, `transformers>=4.30.0`, `keybert>=0.8.0`.

## 5. Files

- **FILE-001**: `src/python/embedding_model.py` — **NEW**. Centralized embedding module wrapping `mlx-embeddings`.
- **FILE-002**: `src/python/main.py` — **MAJOR CHANGES**. Remove torch/sentence-transformers imports, device detection, two model globals. Rewrite `segment_into_paragraphs()` embedding call. Rewrite `compute_religious_embedding()`, `get_religious_relevance()`, `compute_concept_embeddings()`, `get_semantic_themes()`. Gut `extract_tags()` to remove KeyBERT path. Remove `COMMON_STOP_WORDS`, `BIBLE_BOOK_NAMES`, `is_noun()`, `extract_nouns_from_text()`, `RELIGIOUS_SEED_CONCEPTS`, `compute_religious_embedding()`, `get_religious_relevance()`.
- **FILE-003**: `src/python/whisper_bridge.py` — **MODERATE CHANGES**. Remove `_semantic_model`, `_tag_model`, `_device` globals and their accessor functions. Remove `get_device()`. Add `get_embedding_model()`. Update `check_dependencies()`. Verify `segment_paragraphs()` and `extract_tags()` import chains.
- **FILE-004**: `src/python/requirements.txt` — **UPDATE**. Remove torch, sentence-transformers, transformers, keybert. Add mlx-embeddings.
- **FILE-005**: `src/main/services/python-installer.ts` — **MODERATE CHANGES**. Update package verification, install steps, device detection, progress messages.
- **FILE-006**: `src/python/test_embedding_model.py` — **NEW**. Unit tests for embedding module.
- **FILE-007**: `src/python/test_paragraph_segmentation.py` — **NEW**. Integration tests for paragraph segmentation.
- **FILE-008**: `src/python/test_tag_extraction.py` — **NEW**. Integration tests for tag extraction.
- **FILE-009**: `src/python/test_whisper_bridge_deps.py` — **NEW**. Dependency check tests.
- **FILE-010**: `src/python/calibrate_thresholds.py` — **NEW**. Threshold calibration script.
- **FILE-011**: `Model Research Report.md` — **MINOR UPDATE**. Mark Priority 1 as completed.

## 6. Testing

- **TEST-001**: Unit tests for `embedding_model.py` — model loading, single/batch encoding, output shapes, normalization, similarity sanity checks (`test_embedding_model.py`, TASK-033).
- **TEST-002**: Integration tests for paragraph segmentation — basic segmentation, quote boundary preservation, prayer detection, threshold sensitivity (`test_paragraph_segmentation.py`, TASK-034).
- **TEST-003**: Integration tests for tag extraction — theme inference accuracy, KB coverage, max_tags enforcement, quote exclusion, no KeyBERT dependency (`test_tag_extraction.py`, TASK-035).
- **TEST-004**: Dependency check tests — verify `check_dependencies()` reports correct package availability (`test_whisper_bridge_deps.py`, TASK-036).
- **TEST-005**: Regression tests — run all existing Python test files (`test_e2e_pipeline.py`, `test_ast_passage_boundaries.py`, `test_boundary_detection.py`, `test_passage_isolation.py`, `test_remap_boundaries.py`) to verify no breakage (TASK-037).
- **TEST-006**: End-to-end smoke test — full `process_sermon` pipeline with test transcript, verify all stages complete, tags are reasonable, AST builds correctly (TASK-038).
- **TEST-007**: TypeScript test suite — `npm run test:run` to verify no frontend/installer regressions (TASK-039).
- **TEST-008**: Threshold calibration — systematic parameter sweep to find optimal similarity thresholds for the new model (`calibrate_thresholds.py`, TASK-040).

## 7. Risks & Assumptions

- **RISK-001**: EmbeddingGemma-300m similarity distributions may differ significantly from the old models, causing over- or under-segmentation of paragraphs and too many/few tags. **Mitigation**: Phase 9 (TASK-040) includes systematic threshold calibration before finalizing values.
- **RISK-002**: The `mlx-embeddings` package may not fully support the `embeddinggemma-300m-4bit` checkpoint (e.g., missing architecture support or tokenizer incompatibility). **Mitigation**: TASK-002 validates model loading and output shape before proceeding with integration. If the 4bit variant is unsupported, fall back to the bf16 variant.
- **RISK-003**: Batch processing of 200+ sentences for paragraph segmentation may be slow or OOM on machines with limited unified memory. **Mitigation**: TASK-003 adds batch processing with configurable batch size.
- **RISK-004**: Removing PyTorch may break other packages that transitively depend on it. **Mitigation**: NLTK is pure Python. `mlx-whisper` uses MLX. `mutagen` and `requests` are pure Python. Only `sentence-transformers` and `keybert` depend on PyTorch, and both are being removed.
- **RISK-005**: The `mlx-embeddings` `text_embeds` output may not be pre-normalized (L2 norm = 1.0). Current code uses raw cosine similarity with `np.dot / (norm * norm)` which handles unnormalized vectors, so this is safe — but if `text_embeds` is normalized, the similarity math simplifies.
- **RISK-006**: First-time model download (~200MB) may fail or be slow for users with poor connectivity. **Mitigation**: TASK-031 adds pre-download during setup wizard, with progress reporting.

- **ASSUMPTION-001**: The app exclusively targets Apple Silicon Macs. `mlx-embeddings` requires MLX which only runs on Apple Silicon.
- **ASSUMPTION-002**: The `mlx-embeddings` API (`load()` → `model()` → `outputs.text_embeds`) is stable and will not change significantly in upcoming versions.
- **ASSUMPTION-003**: EmbeddingGemma-300m's 768-dim embeddings will produce better or equivalent paragraph boundaries compared to MiniLM's 384-dim embeddings, after threshold retuning.
- **ASSUMPTION-004**: The theological concepts KB (~200 concepts with descriptions) is sufficient for tag extraction without KeyBERT supplementation. This was already the primary method; KeyBERT was secondary.

## 8. Related Specifications / Further Reading

- [Model Research Report.md](../Model%20Research%20Report.md) — Original research document with model benchmarks and recommendations.
- [mlx-embeddings GitHub](https://github.com/Blaizzy/mlx-embeddings) — Package documentation and supported architectures.
- [EmbeddingGemma-300m on HuggingFace](https://huggingface.co/mlx-community/embeddinggemma-300m-4bit) — Model card with benchmark scores and usage.
- [mlx-embeddings API examples](https://github.com/Blaizzy/mlx-embeddings#readme) — Batch encoding, similarity computation patterns.
- [document-model-instructions.md](../.github/document-model-instructions.md) — Document model architecture (unaffected by this migration, but included for context on the AST pipeline).
